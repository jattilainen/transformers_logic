# Как научить трансформеры логике?

Недавние исследования в области анализа работы архитектуры трансформера при решении задач обработки естественного языка показывают, что его решения основывается не на понимании смысла предложения целиком, а на более примитивных представлениях. В нашей работе мы рассматриваем несколько способов заставить трансформер лучше понимать анализируемые предложения, показываем, что наивные подходы могут даже усугубить проблему, и предлагаем новый способ дообучения модели RoBERTa на конечной задачи с использованием вспомогательных задач на промежуточных слоях. Параллельно, мы анализируем влияния различных метрик похожести трансформера на человека на итоговую точность модели. Мы приходим к выводу, что повышенная чувствительность к порядку слов помогает при решении конечной задачи, а точность на состязательных и контрастных наборах данных, а также контекстуальность векторных представлений слабо скоррелированы с итоговой точностью модели.

В данном репозитории представлен код для обучения всех используемых моделей, а также код для подсчета необходимых метрик. Для подготовки наборов данных следует использовать файлы cline/preprocess_dataset.py и human_eval/preprocess_dataset.py, для обучения моделей использовать файлы train.py из соответсвующих директорий (описание параметров доступно по команде --help), а для подсчета метрик использовать файл human_eval/eval.py .
